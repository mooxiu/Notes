\chapter{Symmetric Matrices}

\begin{definition}
    A \textbf{Symmetric} matrix is a matrix A such that $A^{T} = A$.
\end{definition}


\section{Diagonalization of symmetric matrix}

Recall that \textbf{linear independent} does not mean \textbf{orthogonal}. If we say 2 vectors are linear independent, which simply means no scalar c except for 0 can make $v_1 = c v_2$, while orthogonal means the dot product of 2 vectors is zero: $v_1 \perp v_2 \Rightarrow v_1 \cdot v_2 = 0$.

Look carefully at following example:
\begin{eg}
    diagonalize the matrix $A = \begin{bmatrix}
        6 & -2 & -1 \\
        -2 & 6 & -1 \\
        -1 & -1 & 5 \\
    \end{bmatrix}$

    The result is $A = PDP^{-1}$, where $P = \begin{bmatrix}
        -1/\sqrt{2} & -1/\sqrt{6} & 1/\sqrt{3} \\
        1/\sqrt{2} & -1/\sqrt{6} & 1/\sqrt{3} \\
        0 & 2/\sqrt{6} & 1/\sqrt{3} \\
    \end{bmatrix}$, $D = \begin{bmatrix}
        8 & 0 & 0 \\
        0 & 6 & 0 \\
        0 & 0 & 3 \\
    \end{bmatrix}$.
\end{eg}

In the above example, because P is square and has orthogonal columns, according to what we have proved in orthogonal section, P is an \textit{orthogonal} matrix, and $P^{-1} = P ^T$.


\begin{remark}
    Does a symmetric matrix necessarily diagonalizable?
\end{remark}

The answer is yes, will prove this in following theorems. Also, intuitively, a symmetric matrix is similar with a diagonal matrix.

\begin{remark}
    Does a symmetric matrix necessarily has eigenvalues?
\end{remark}

The answer is yes, and we can depend this on the previous question: because it is diagonalizable, then it must have eigenvalues and eigenvectors.

\begin{theorem}\label{theorem: 7.1.1}
    If A is symmetric, then any 2 eigenvectors from different \\ eigenspaces are orthogonal.
\end{theorem}

\begin{proof}
    Suppose matrix A has $\lambda_1, \lambda_2$ as eigenvalues and $v_1, v_2$ as eigenvectors.

    \begin{align*}
        \lambda_1v_1 \cdot v_2 & = (\lambda_1 v_1)^{T} v_2 \qquad(obviously)\\
        & = (Av_1)^T v_2 \qquad(definition of eigen)  \\
        & = v_1^T A^T v_2 \qquad(transpose) \\
        & = v_1^T A v_2 \qquad(definition of symmetric) \\
        & = v_1^T \lambda_2 v_2 \qquad(eigen) \\
        & = \lambda_2 v_1 \cdot v_2
    \end{align*}

    $$\lambda_1 \neq \lambda_2 \Rightarrow v_1 \cdot v_2 = 0$$
\end{proof}

\begin{theorem}\label{theorem: 7.1.2}
    An $n \times n$ matrix A is orthogonally diagonalizable if and only if A is a symmetric matrix.
\end{theorem}

\begin{proof}[Part of proof]
One part of this theorem is easy to prove: if it's orthogonally diagonalizable then it is a symmetric matrix:

(1) What is \textbf{orthogonally diagonalizable}?

An $n \times n$ matrix A is said to be \text{orthogonal diagonalizable} if there are an orthogonal matrix P (a real square matrix whose columns and rows are orthonormal vectors) (with $P^{-1} = P^T$) and a diagonal matrix D such that:
$$A = PDP^T = PDP^{-1}$$

(2) How can we prove such A is symmetric?

Always remember by definition we're going to prove $A^T = A$. Let's apply transpose to A:

$$A^T = (PDP^T)^T = P^{TT}DP^T = PDP^T = A$$
\end{proof}

Another part of the theorem: if symmetric then orthogonally diagonalizable is hard to proof, the book omitted it here.

% ............the end of section............

\section{The Spectral Theorem}

The set of eigenvalues of matrix A is sometimes called the \textit{spectrum} of A.

\begin{theorem}[The Spectral Theorem for Symmetric Matrices]
An \(n \times n\) symmetric matrix A has the following properties:

a. A has n real eigenvalues, counting the multiplicities

b. The dimension of the eigenspace for each eigenvalues \(\lambda \) equals the multiplicities of \(\lambda \) as a root of the characteristic equation

c. The eigenspaces are mutually orthogonal, in the sense that eigenvectors corresponding to different eigenvalues are orthogonal

d. A is orthogonal diagonalizable
\end{theorem}

\begin{proof}
    a. To find eigenvalues for A, we have \(Ax = \lambda x\), to find x, we have \(det(A - \lambda I)= 0\). Because it is an n degree polynomial equation, according to the foundational theorem of algebra, it has n roots.
    
    According to \hyperref[ex: 5.5.24]{this exercise}, \(\lambda\) will be real number, thus we can say it has n real eigenvalues. 

    c. The same with \hyperref[theorem: 7.1.1]{this theorem}.

    d. See \hyperref[ex: 7.1.31]{this exercise}

    b. Follows d.
\end{proof}

\begin{remark}
    What is eigenspace?

    Eigenspace is all solutions of \((A - \lambda I) \vec{x}  = 0\) 
\end{remark}


\begin{remark}
    What is the dimension of eigenspace?

    How many vectors we need to span the eigenspace.
\end{remark}

\begin{problem}[chapter 7.1 exercise 31]\label{ex: 7.1.31}
    Let \(A = PDP^{-1}\), where P is orthogonal and D is diagonal, and let \(\lambda\) be an eigenvalue of A of multiplicity k. Then \(\lambda\) appears k times on the diagonal of D. Explain why the dimension of the eigenspace for \(\lambda\) is k.   
\end{problem}
\begin{proof}
    According to \hyperref[theorem: diagonal theorem]{diagonal theorem}, because A is diagonalizable, implies that P are linear independent eigenvectors of A corresponding to eigenvalues.

    So, P has k eigenvectors corresponding to \(\lambda\). 
\end{proof}


% Quadratic forms, corresponding to chapter 7.2 of the book
\section{Quadratic Forms}

In general, \textbf{quadratic form} is a polynomial with terms all of degree two (\textbf{form}  is another name for a homogeneous polynomial). A homogeneous polynomial is a polynomial whose nonzero terms all have the same degree.

Here is the definition on linear algebra:
\begin{definition}[quadratic form]
    A \textbf{quadratic form} on \(\R^n\) is a function Q defined in \(\R^n\) whose value at a vector x in \(\R^n\) can be computed by an expression of the form \(Q(x) = x^T A x\), where A is an \(n \times n\)  symmetric matrix.      
    The matrix A is called the \textbf{matrix of the quadratic form}. 
\end{definition}

\begin{eg}
    The simplest example of a nonzero quadratic form is \(Q(x) = x^T I x = {\lVert x \lVert}^2\). 
\end{eg}

\subsection{Change of Variable in a Quadratic Form}
\begin{definition}[change of variable]
    If x represents a variable vector in \(R^n\), then a \textbf{change of variable} is an equation of the form

    \(x = Py\) or equivalently, \(y = P^{-1}x\)  \\
    where P is an invertible matrix and y is a new variable vector in \(R^n\). Here y is the coordinate vector of x relative to the basis of \(R^n\) determined by the columns of P. 
\end{definition}
\begin{remark}
    If the change of variable is made in a quadratic form \(x^T Ax\), then
    \[
        x^TAx = (Py)^T A (Py) = y^T P^T APy = y^T(P^TAP)y  \tag{1}
    \]
    and the new matrix of the quadratic form is \(P^T AP\). 
    Since A is symmetric, and \hyperref[theorem: 7.1.2]{this theorem} shows that A can be decomposed of \(P DP^T\), thus we have a diagonal matrix \(D = P^T AP\). 

    The quadratic form (1) becomes \(y^TDy\). 
\end{remark}




