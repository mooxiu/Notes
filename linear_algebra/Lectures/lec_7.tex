\chapter{Symmetric Matrices}

\begin{definition}
    A \textbf{Symmetric} matrix is a matrix A such that $A^{T} = A$.
\end{definition}


\section{Diagonalization of symmetric matrix}

Recall that \textbf{linear independent} does not mean \textbf{orthogonal}. If we say 2 vectors are linear independent, which simply means no scalar c except for 0 can make $v_1 = c v_2$, while orthogonal means the dot product of 2 vectors is zero: $v_1 \perp v_2 \Rightarrow v_1 \cdot v_2 = 0$.

Look carefully at following example:
\begin{eg}
    diagonalize the matrix $A = \begin{bmatrix}
        6 & -2 & -1 \\
        -2 & 6 & -1 \\
        -1 & -1 & 5 \\
    \end{bmatrix}$

    The result is $A = PDP^{-1}$, where $P = \begin{bmatrix}
        -1/\sqrt{2} & -1/\sqrt{6} & 1/\sqrt{3} \\
        1/\sqrt{2} & -1/\sqrt{6} & 1/\sqrt{3} \\
        0 & 2/\sqrt{6} & 1/\sqrt{3} \\
    \end{bmatrix}$, $D = \begin{bmatrix}
        8 & 0 & 0 \\
        0 & 6 & 0 \\
        0 & 0 & 3 \\
    \end{bmatrix}$.
\end{eg}

In the above example, because P is square and has orthogonal columns, according to what we have proved in orthogonal section, P is an \textit{orthogonal} matrix, and $P^{-1} = P ^T$.


\begin{remark}
    Does a symmetric matrix necessarily diagonalizable?
\end{remark}

The answer is yes, will prove this in following theorems. Also, intuitively, a symmetric matrix is similar with a diagonal matrix.

\begin{remark}
    Does a symmetric matrix necessarily has eigenvalues?
\end{remark}

The answer is yes, and we can depend this on the previous question: because it is diagonalizable, then it must have eigenvalues and eigenvectors.

\begin{theorem}\label{theorem: 7.1.1}
    If A is symmetric, then any 2 eigenvectors from different \\ eigenspaces are orthogonal.
\end{theorem}

\begin{proof}
    Suppose matrix A has $\lambda_1, \lambda_2$ as eigenvalues and $v_1, v_2$ as eigenvectors.

    \begin{align*}
        \lambda_1v_1 \cdot v_2 & = (\lambda_1 v_1)^{T} v_2 \qquad(obviously)\\
        & = (Av_1)^T v_2 \qquad(definition of eigen)  \\
        & = v_1^T A^T v_2 \qquad(transpose) \\
        & = v_1^T A v_2 \qquad(definition of symmetric) \\
        & = v_1^T \lambda_2 v_2 \qquad(eigen) \\
        & = \lambda_2 v_1 \cdot v_2
    \end{align*}

    $$\lambda_1 \neq \lambda_2 \Rightarrow v_1 \cdot v_2 = 0$$
\end{proof}

\begin{theorem}\label{theorem: 7.1.2}
    An $n \times n$ matrix A is orthogonally diagonalizable if and only if A is a symmetric matrix.
\end{theorem}

\begin{proof}[Part of proof]
One part of this theorem is easy to prove: if it's orthogonally diagonalizable then it is a symmetric matrix:

(1) What is \textbf{orthogonally diagonalizable}?

An $n \times n$ matrix A is said to be \text{orthogonal diagonalizable} if there are an orthogonal matrix P (a real square matrix whose columns and rows are orthonormal vectors) (with $P^{-1} = P^T$) and a diagonal matrix D such that:
$$A = PDP^T = PDP^{-1}$$

(2) How can we prove such A is symmetric?

Always remember by definition we're going to prove $A^T = A$. Let's apply transpose to A:

$$A^T = (PDP^T)^T = P^{TT}DP^T = PDP^T = A$$
\end{proof}

Another part of the theorem: if symmetric then orthogonally diagonalizable is hard to proof, the book omitted it here.

% ............the end of section............

\section{The Spectral Theorem}

The set of eigenvalues of matrix A is sometimes called the \textit{spectrum} of A.

\begin{theorem}[The Spectral Theorem for Symmetric Matrices]
An \(n \times n\) symmetric matrix A has the following properties:

a. A has n real eigenvalues, counting the multiplicities

b. The dimension of the eigenspace for each eigenvalues \(\lambda \) equals the multiplicities of \(\lambda \) as a root of the characteristic equation

c. The eigenspaces are mutually orthogonal, in the sense that eigenvectors corresponding to different eigenvalues are orthogonal

d. A is orthogonal diagonalizable
\end{theorem}

\begin{proof}
    a. To find eigenvalues for A, we have \(Ax = \lambda x\), to find x, we have \(det(A - \lambda I)= 0\). Because it is an n degree polynomial equation, according to the foundational theorem of algebra, it has n roots.
    
    According to \hyperref[ex: 5.5.24]{this exercise}, \(\lambda\) will be real number, thus we can say it has n real eigenvalues. 

    c. The same with \hyperref[theorem: 7.1.1]{this theorem}.

    d. See \hyperref[ex: 7.1.31]{this exercise}

    b. Follows d.
\end{proof}

\begin{remark}
    What is eigenspace?

    Eigenspace is all solutions of \((A - \lambda I) \vec{x}  = 0\) 
\end{remark}


\begin{remark}
    What is the dimension of eigenspace?

    How many vectors we need to span the eigenspace.
\end{remark}

\begin{problem}[chapter 7.1 exercise 31]\label{ex: 7.1.31}
    Let \(A = PDP^{-1}\), where P is orthogonal and D is diagonal, and let \(\lambda\) be an eigenvalue of A of multiplicity k. Then \(\lambda\) appears k times on the diagonal of D. Explain why the dimension of the eigenspace for \(\lambda\) is k.   
\end{problem}
\begin{proof}
    According to \hyperref[theorem: diagonal theorem]{diagonal theorem}, because A is diagonalizable, implies that P are linear independent eigenvectors of A corresponding to eigenvalues.

    So, P has k eigenvectors corresponding to \(\lambda\). 
\end{proof}


% Quadratic forms, corresponding to chapter 7.2 of the book
\section{Quadratic Forms}

In general, \textbf{quadratic form} is a polynomial with terms all of degree two (\textbf{form}  is another name for a homogeneous polynomial). A homogeneous polynomial is a polynomial whose nonzero terms all have the same degree.

Here is the definition on linear algebra:
\begin{definition}[quadratic form]
    A \textbf{quadratic form} on \(\R^n\) is a function Q defined in \(\R^n\) whose value at a vector x in \(\R^n\) can be computed by an expression of the form \(Q(x) = x^T A x\), where A is an \(n \times n\)  symmetric matrix.      
    The matrix A is called the \textbf{matrix of the quadratic form}. 
\end{definition}

\begin{eg}
    The simplest example of a nonzero quadratic form is \(Q(x) = x^T I x = {\lVert x \lVert}^2\). 
\end{eg}

\subsection{Change of Variable in a Quadratic Form}
\begin{definition}[change of variable]
    If x represents a variable vector in \(R^n\), then a \textbf{change of variable} is an equation of the form

    \(x = Py\) or equivalently, \(y = P^{-1}x\)  \\
    where P is an invertible matrix and y is a new variable vector in \(R^n\). Here y is the coordinate vector of x relative to the basis of \(R^n\) determined by the columns of P. 
\end{definition}
\begin{remark}
    If the change of variable is made in a quadratic form \(x^T Ax\), then
    \[
        x^TAx = (Py)^T A (Py) = y^T P^T APy = y^T(P^TAP)y  \tag{1}
    \]
    and the new matrix of the quadratic form is \(P^T AP\). 
    Since A is symmetric, and \hyperref[theorem: 7.1.2]{this theorem} shows that A can be decomposed of \(P DP^T\), thus we have a diagonal matrix \(D = P^T AP\). 

    The quadratic form (1) becomes \(y^TDy\). 
\end{remark}

\begin{eg}
    Do the change of variable on \(A = \begin{bmatrix}
        1 &  -4 \\
        -4 &  5 \\
    \end{bmatrix}\) 

    1. First let's see the quadratic form. \(\vec{x} = \begin{bmatrix}
        x_1 &  x_2 \\
    \end{bmatrix}\): 
    \[
        x^TAx = x_1^2 - 8x_1x_2 - 5x_2^2
    \]

    2. Try to find P, so we can do the change of variable.

    The eigenvalues and corresponding eigenvectors are:
    \[
        \lambda = 3: \begin{bmatrix}
             2/\sqrt{5} \\
             1/\sqrt{5} \\
        \end{bmatrix};
        \quad
        \lambda = -7: \begin{bmatrix}
             1/\sqrt{5} \\
             2/\sqrt{5} \\
        \end{bmatrix}
    \]

    \begin{remark}
    Notice that we need to normalize the eigenvector, so we have \(P^T = P^\{ -1 \} \).
    \end{remark}

    So we have \(P = \begin{bmatrix}
        2/\sqrt{5} &  1/\sqrt{5} \\
        -1/\sqrt{5} &  2/\sqrt{5} \\
    \end{bmatrix}\)
    and  \(D = \begin{bmatrix}
        3 &  0 \\
        0 &  -7 \\
    \end{bmatrix}\) 

    3. Replace \(x = Py\) and calculate \(x^TAx = (Py)^TA(Py) = y^TP^TAPy = y^TDy = 3y_1^2 - 7 y_2^2\)   

    That's the end of the change of variable.
\end{eg}

\begin{note}
    Quadratic form is very natural in real day problems. 

    We should know that \(Q(x) = x^TAx\)  is simply \(Q(x) = \sum_{i=1}^n \sum_{j=1}^n a_{ij}x_ix_j\). 

    And change of variable makes the calculation easier.

    I would think that this subsection is a small application for symmetric matrices.

    Also notice the change of variable has no cross-product term.
\end{note}

The above example has shown that:
\begin{theorem}[The Principle Axes Theorem]\label{theorem: Principle Axes Theorem}
    Let A be an \(n \times n\) symmetric matrix. Then there is an orthogonal change of variable, \(x = Py\), that transforms the quadratic form \(x^TAx\) into a quadratic form \(y^TDy\) with no cross-product term.    
\end{theorem}

\subsection{A Geometric View of Principle Axes}
For a general form of \(Q(x) = x^TAx\), suppose \(A = \begin{bmatrix}
    a_{11} &  a_{12} \\
    a_{21} &  a_{22} \\
\end{bmatrix}\), we have \(Q(x) = a_{11} x_1^2 + (a_{12}+1_{21})x_1x_2 + a_{22}x_2^2\).  

Let \(c\) be a constant and let's show all x in \(\R^2\) satisfy \(Q(x) = c\):  

If \(A\) is a diagonal matrix, then we can always get the form of an ellipse (\(\dfrac{x_1^2}{a^2} + \dfrac{x_2^2}{b^2} = 1, a > b > 0\)) or a hyperbola (\(\dfrac{x_1^2}{a^2} - \dfrac{x_2^2}{b^2} = 1, a > b > 0\)):
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{standard-ellipse-hyperbola}
    \caption{An ellipse and a hyperbola in standard position}
    \label{fig:standard-ellipse-hyperbola}
\end{figure}

If \(A\)  is not a diagonal matrix, then the graphs will be rotated out of the standard position:
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{non-standard-ellipse-and-hyperbola}
    \caption{An ellipse and a hyperbola not in standard position}
    \label{fig:non-standard-ellipse-and-hyperbola}
\end{figure}

\subsection{Classifying Quadratic Forms}
When A is an \(n \times n\) matrix, the quadratic form \(Q(x) = x^T Ax\) is a real-valued function with domain \(R^n\).   

\begin{definition}
    A quadratic form Q is:
    \begin{itemize}
        \item \textbf{positive definite} if \(Q(x) > 0\) for all \(x \ne 0\). 
        \item \textbf{negative definite} if \(Q(x) < 0\) for all \(x \ne 0\). 
        \item \textbf{indefinite}  if \(Q(x)\) assumes both positive and negative values.  
    \end{itemize}
\end{definition}

Also, Q is said to be \textbf{positive semidefinite} if \(Q(x) \geq 0\) for all x, and to be \textbf{negative semidefinite} if \(Q(x) \leq 0\) for all x.    

\begin{theorem}[QUadratic Forms and Eigenvalues]
    Let A be an \(n \times n\) symmetric matrix. Then a quadratic form \(x^TAx\) is:
    \begin{itemize}
        \item positive definite if and only if the eigenvalues of A are all positive,
        \item negative definite if and only if the eigenvalues of A are all negative,
        \item indefinite if and only if A has both positive and negative eigenvalues.
    \end{itemize} 
\end{theorem}
\begin{proof}
    According to \hyperref[theorem: Principle Axes Theorem]{Principle Axes Theorem}, there exists an orthogonal change of variable \(x = Py\) such that:
    \[
        Q(x) = x^TAx = y^T Dy = \lambda_1 y_1^2 + \lambda_2 y_2^2 + \cdots + \lambda_n y_n^2
    \]
    where \(\lambda_1, \lambda_2, \cdots, \lambda_n\) are the eigenvalues of A. 
    Since P is invertible, there's a one-to-one correspondence between all nonzero x and all nonzero y.
\end{proof}

\begin{eg}
    Is \(Q(x) = 3x_1^2 + 2x_2^2 + x_3^2 + 4x_1x_2 + 4x_2x_3\) positive definite?

    We can easily find its matrix is \(A = \begin{bmatrix}
        3 & 2 &  0 \\
        2 & 2 &  2 \\
        0 & 2 &  1 \\
    \end{bmatrix}\), and its eigenvalues are 5, 2 and -1. So Q is an indefinite quadratic form, not positive definite. 
\end{eg}

The classification of a quadratic form is often carried over to the matrix of the form. Thus:
\begin{definition}
    A \textbf{positive definite matrix} A is a \textit{symmetric} matrix for which the quadratic form \(x^TAx\) is positive definite. Other terms, such as \textbf{positive semidefinite matrix}, are defined analogously.  
\end{definition}

\begin{note}
    A fast way to determine whether a symmetric matrix A is positive definite is to attempt to factor A in the form \(A = R^TR\), where R is upper triangular with positive diagonal entries. 
    (A slightly modified algorithm for an LU factorization is one approach.)
    Such a \textit{Cholesky factorization} is possible if and only if A is positive definite.
\end{note}

\section{The singular value decomposition}

